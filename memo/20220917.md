
戦略整理

- mv portfolio
- 単一銘柄
- 2銘柄
- 複数銘柄

単一銘柄

- symbol: eth
- horizon: 4
- model: lgbm
- y: raw

2銘柄

- symbol: btc/eth
- horizon: 24
- model: lgbm
- y: beta

複数銘柄

- horizon: 12, 24, 48, 96
- model: ridge, lgbm, nn
- y: beta, rank, ranker

アイデア

- 過去のbfモデルやftx複数銘柄の特徴量を使ったらどうなる？
- minor銘柄(少し混ぜるくらいなら)
- doge戦略
- universalとの相関が低いモデルをforkして改善 (m-20220904-nn, m-20220907-beta-rank, m-20220910-rank)
- 最強を改善 (m-20220910-beta-fix2)
- momentum戦略 DONE

TODO

- bfモデルベース
- ftx複数銘柄ベース


アイデア

- 時刻を考慮しないでval lossを下げる解: 一般的なkaggleのテクニックとかで性能上げる
- ファクターを均等に混ぜる解: colsample_bytreeを下げたり、ridgeの正則化を強くしたり
- 安定したファクターを使う解: double sharpeとかで特徴量選択
- ファクターを均等に混ぜるだけだと、新しいファクターを作れない気がする
- 非線形に学習させた解を混ぜたほうが良い気がする

アンサンブル (see 20220918_eda)

- baggingしないとばらつきが多いので、bagging後でアンサンブル効果があるか調べる必要がある
- positionでアンサンブルするのが良さそう(別モデルとしてデプロイ)
- 理由は3つ
- 1: 性能の保証がある (リターンはモデル平均。分散はモデル平均以下になる)
- 2: 実験的にpositionでアンサンブルしたほうがsharpeが高いことが多い
- 3: positionと取引量が減る
- pos_typeは多様性あまり無いけど、rankとbetaを平均するとsharpeが少し上がるときがある
- pos_type sharpe: rank < beta, rank + beta, normalize(rank + beta) 
- pos_type = betaのほうが、リターンは大きいから、betaで良いかも
- lgbmとHistGradientBoostingRegressorは多様性作れるけど、HistGradientBoostingRegressorの性能が低いだけかも
- y_typeは多様性作れるけど性能差があるから本当に効いているか不明
- lgbmとlgbm_clfはy_typeほどではないけど多様性作れる
- lgbmとlgbm_etは少し多様性作れるけど、bagging数増やすのと大差無いかも
- y_type beta, beta_simpleは多様性作れる
- objective l2, l1はあまり多様性作れないし、l1で性能落ちる
- colsample_bytreeは多様性作れる
- feature_unionはあまり多様性作れない
- y_type rank, rank_weightは多様性少し作れる
- y_type rank_beta_weightはbetaと似ている。ほぼ同じ性能で、少し多様性作れる
- y_type rank_beta_mean_weightは性能低い
- y_transform uniformは性能低い

アンサンブル方針

- 1つのモデルでアンサンブルする必要は無い (universal portfolioに任せる。baggingなどは良い)
- pos_type, y_type, 非線形度合い(colsample_bytree)で多様性作れる
- 現状のポートフォリオで足りないモデルを手作りが良さそう (モデルが増えると管理が大変なので)

numerai TCについて

- https://zenn.dev/katsu1110/articles/b517135ff42235
- online learningとの関係は？
- SAAだと他の予測値の影響を重みの分母でしか受けない
- SAAと似ている気がする。分母が報酬に影響するようにしたのがポイントかな

dcor

- 計算が遅い
- 時刻依存性を捉えられそう

rankについて

- 銘柄数が変わると分布が変わってしまう (時刻dcorが大きくなる)
- 乱数で銘柄数に依存させなくする？
- ついでにtest time data aug?
- 銘柄数を考慮せずに乱数足したら性能上がった気がする
- 考慮して一様にしても性能上がるわけでは無さそう
- 普通にnormal乱数を足すのが良さそう

パフォーマンス

- df[features] += noiseが遅かった。df._dataを見て、全てfloat64にして一つのブロックになるようにしたら速くなった
- df.loc[:, features] += noiseは常に遅い
- 重い計算はjoblib.memoryを使うことにした。本番では使わないように環境変数で分岐する (see 20220919_rank)

アイデア

- 銘柄に重み付けしたら？BTCが儲かりやすいモデルとかのアンサンブル

アイデア

- sunday rsi -> 性能落ちた。boundedになるから扱いやすそうだけど (see 20220921_eth_sunday_rsi_exp.ipynb)
- y rsi -> 良い。銘柄間rankを取らなくても学習できるから、バリエーション作れそう

執行モデル (20220923_exec)

- BTCで1/6, MATICで1/4くらいコスト削減できそう
- 5分だと半分以上削減
- 全期間安定しているから、チューニングの余地がかなりありそう
- ↑は先読み(執行を早める)と遅延(執行を遅くする)を合わせたもの
- 先読みと遅延片方だけでもコスト削減できそう
- 理想リターンよりもfepリターンのほうが予測しやすい気がする
- 理想リターンは執行遅延させてもあまり成績劣化しない (特徴量の時間軸が6以上だからか？)

TODO

- universalポートフォリオのレバレッジ上げたやつ追加
- mean varianceポートフォリオを試しに追加してみる (一旦、モニタリング)
- 執行モデルをアンサンブルするシステムできないか？
- 執行モデル研究

執行モデルをアンサンブル (twitterコピー)

長い時間軸のポジション微分の符号が変わらないように短い時間軸戦略を足すと、短い時間軸戦略はコストゼロでリターンだけ得られる
微分符号が変わらないみたいな条件は複雑になるから。シンプルに、短い時間軸のポジション微分が、長い時間軸のポジション微分と同じくらいか、少し小さいくらいになるようにすれば、近似的にこれを達成できる気がする
仮に長期でtwapで買ってるときに、短い時間軸戦略を混ぜると、ポジション微分は
p_long'(t) + p_short'(t)
でp_long'(t)はその場所ではほぼ一定だから
c + p_short'(t)
|p_short'(t)| <= cならok
方針1:
|p_short'(t)| <= cが満たされるように、p_shortのスケールを自動調整
方針2:
執行モデルとして特別に処理する
方針3:
コスト負けしないギリギリの時間軸のモデルを作り、普通にアンサンブルする (長期よりもシャープが高いから、重み調整は不要で、スケールする限界まで混ぜれば良い)
方針3が良い気がする。時間軸長くしてるのは、コストもあるけど、スケールが大きい。スケール無視すれば、時間軸短くできるはず。コストに勝てるなら方針3で良いし、負けるなら方針1に使える
現状のシステムが短い時間軸想定してないから、ちょっと考えないと。DBとか大丈夫かな。cloud run jobsは、cloud runかgceにしないと。設定ファイルからデプロイできるようにしたい
容量
10モデル * 1分足 * 1年 * 10銘柄 = 52M row
現状の長い時間軸
100モデル * 5分足 * 1年 * 10銘柄 = 104M row
大差無いか
短い時間軸は別トーナメントにするか。普通にアンサンブルしたら重みが大きくなりすぎるから。別にして、最後に短い時間軸と長い時間軸をアンサンブルする
そのやり方が方針1か方針3かは柔軟にできる
どの方針でも実験時はコスト考慮やわざと執行遅延が必要そう (予測しやすいときはコストも上がる)

アンサンブルについて(20220923_pos_ensemble)

- 寄与が大きいのは長い時間軸 (取引量を揃えた場合、短い時間軸はポジションが小さくなるので)
- 下位時間軸をアンサンブルするとリターンとシャープレシオが最大3倍くらいに上がる
- 上位時間軸をアンサンブルするとシャープレシオが下がる
- 執行戦略レベルの時間軸で工夫しても寄与は小さい (それで長い時間軸の取引コストくらいの規模で儲かるなら苦労しない)
- その時間軸の割合が小さければ取引コストは無視できる (周波数に被りの無い戦略の取引量の比が小さいと、取引量削減率が大きい)
- スケール的にも取引コスト削減的にも、各時間軸の取引量を同じくらいにするのが良さそう (ポジションが時間軸に反比例する)

方針2(執行を前後にずらす)よりも方針3が良い理由

- numeraiのルールだとしたら方針3のほうが人が参加しそう (シンプル)
- 方針3は改善ループが回しやすい (改善ループを回すことが最も重要)
- 方針3に対する方針2のゲインは、最も短い時間軸の取引コストをゼロにできること
- 方針3なら色々な時間軸をアンサンブルしやすい
- 方針2だと執行を前後にずらすモデルと予測モデルの干渉を考えないといけなくなる (干渉しないくらい長い時間軸しかできなくなる)

踏まえて方針

- メインの時間軸からだんだん時間軸を短くしながら研究する (12時間や6時間など)
- ポートフォリオ最適化で取引コストや時間軸を考慮する
- (取引量が一定以下になるように補正するとか)
- (取引量が減るような比率で混ぜるとか)
- 分足などで研究 (勝てるかもしれない)

優先順位的には、ただ時間軸を短くしながら研究するのがコスパが良い気がする。
5分足より短くなったり、5分足でもtwap執行でなくなると、
少し条件が変わるから懸念が増える。
今のルールの範囲での改善が良さそう

wavelet特徴量は効かなかった (20220924_wavelet.ipynb)

g-researchのsolutionを試す

- https://www.kaggle.com/code/sudalairajkumar/winning-solutions-of-kaggle-competitions
- これ面白そう https://www.kaggle.com/c/g-research-crypto-forecasting/discussion/323250

NN (20220924_nn_portfolio)

- dropout 0.5だとmc dropoutしないと誤差が大きいのでは？
- epochsを増やすと過学習するらしい (正則化でも完全には防げない)
- lossをいじると性能上がる
- mc dropoutを試したら大差無かった (非線形度合いが強くなると効いたりしないかな)

DONE

- 特徴量をさらに追加 (pandas_ta) DONE
- 板データ DONE
- 天気を調べる -> 株の先行研究的には効かないらしい

TODO

- lgbm custom loss (時刻単位でまとめると良い気がする)
- lgbm tuning (大きいcolsample_bytreeで学習して非線形アルファを得られないか？)
- 月を調べる
- コモディティーを調べる
- 通貨を調べる

メモ

- pandas_taで特徴量追加したら少し成績上がった (20220924_add_feat)
- orderbookとquoteデータ追加したら少し成績上がった(わずか。pandas_taで上がるのと同じくらい) (20220926_ob)
- liqデータ追加したら成績上がらない (20220926_ob)
- 結論、ob, quote, liqは使わずにテクニカル指標追加が良さそう

アイデア

- beta, rankの他に、btcをやったらどうなる？(btcとの相対値を特徴量にする。btc) 
- -> 成績良いかも。betaより安定している気がする
- 銘柄間の処理が重要なのでは？株でも業種の処理が重要らしい

visualize_resultのこの行は、horizonによって変える必要がある
df["position_prev"] = df.groupby(["hour", symbol_col])["position"].shift(1).fillna(0)

アイデア

- visualize_resultのhorizon修正
- talibに価格以外のものを入力
- umap特徴量 (入力が[0, 1]だと良いらしい)
- online pca
- normalize + beta one
- y = sharpe
- eth beta -> ぱっとしない

バックテストとの一致

- example_model_rankはほぼ一致。微妙にずれがある (index price)
- 20220907_betaはかなり一致。ところどころパルス状にずれがある (index price)
- 20220908_rankerはかなり一致。(market price)
- remove_featはかなりずれがある。特徴量計算のせいかも
- 20220910_betaはかなり一致するから、特徴量計算ではなさそう。feature removerか？
- 20220910_beta_fix2もかなり一致。feature removerかjoblib memoryな気がする
- 20220919_rankは一致(joblib memoryのみ使ってる。特徴量はソート済みfloat32)
- 20220911_beta_h12は一致。horizonは問題無い
- feature removerかpipelineが原因な気がする

一致しないもの

- 0907_eth: 
- 0921_eth: pipeline
- remove_feat: feature remover, pipeline

todo

- 一気に全モデルをテストできるようにする。モデルごとの誤差を表示してみて、法則を見つける
- git checkout dataしてないだけだった

flamlでチューニング

- 評価指標は相関の幾何平均か調和平均が良さそう
- n_estimatorsの最小値を大きくすると汎化性能が上がる気がする
- baggingでwrapするのも良い気がする
- 小さいn_estimators + baggingも良いのでは？
- 特徴量によって変わる。特徴量もチューニングできないか？
- btc単一もやってみたい

feature gradient selector

- 古くて動かないのでコピーして修正
- 計算中。かなり時間かかる
- 普通の特徴量選択と大差無さそう

TODO

- double sharpeを表示 -> btc betaが高い
- betaとbtc betaをflamlや特徴量追加で改善 -> flamlは改善しなかった

アイデア

- チューニングよりも手作業のほうが性能が高い
- モデル改善の余地があるのでは？
- 1dcnnが効く理由は周波数空間での結合がsparseになるから？

TODO

- 1dcnn(2d): https://gist.github.com/richmanbtc/4249146150637ab51565f5b821b8ae26
- 1dcnn(3d)の検証 -> 1dcnn(2d)より良くない
- 1dcnn(2d) + bias=False -> 1dcnn(2d)より良くない。偶然？
- 1dcnn(2d) + orthogonal init -> 1dcnn(2d)より良くない
- dense + l1 reg -> 1dcnn(2d)くらいの成績 https://gist.github.com/richmanbtc/e159d0ca1ff583e053688b8b8dd1fd7a https://gist.github.com/richmanbtc/4432c87144afe64828ca22387e603ee9
- dense + sparse constraint -> dense + l1 regより成績良く無い https://gist.github.com/richmanbtc/1b953d374bb979a9306ccac8d9a1a6e4
- 1dcnn(2d) + full kernel size + repeat padding -> paddingはsameしかなかったのでsameで -> 成績落ちた
- 1dcnn(2d) + kernel size 0.5 -> 1dcnn(2d)より成績悪い https://gist.github.com/richmanbtc/3149c4b6878cf64fd405f8ddcc7ba7aa
- learning rate変更して1dcnn系を追試 (learning rateの効き方が変わっているだけかの検証) -> lr下げたけど成績上がらない
- 1dcnn(2d) + orth reg
- 1dcnn(SeparableConv2D)
- baggingした場合、dense + l1 regが成績良かった。1dcnn系は偶然かも？

sparse

- https://arxiv.org/pdf/2010.01048.pdf
- https://developer.smartnews.com/blog/2017/06/sparse-neural-network/

https://www.acceluniverse.com/blog/developers/2020/01/deep-double-descent-where-bigger-models-and-more-data-hurt.html
https://www.kaggle.com/questions-and-answers/298935

noiseが多いとdouble descentが始まる時期がかなり遅れるらしい。
トレードのように50%に近いとどうなる？
早めることはできない？
サンプル数の少なさもnoiseの多さと同じような影響を与えてる気がする
optimal early stoppingは単調に下がるっぽいから、
early stoppingで実験するのが良さそう

L1正則化がかけられているとすると、
実質のネットワークサイズは増えていない可能性もありそう
https://arxiv.org/pdf/2003.01897.pdf
https://www.jstage.jst.go.jp/article/pbsj/49/0/49_28/_pdf

batch size

- https://arxiv.org/pdf/1705.10694.pdf
- https://wandb.ai/wandb_fc/japanese/reports/---Vmlldzo1NTkzOTg
- https://www.jstage.jst.go.jp/article/pjsai/JSAI2020/0/JSAI2020_4Rin169/_pdf/-char/ja
- effective batch size
- linear scaling rule https://www.baeldung.com/cs/learning-rate-batch-size
- 達成できる相関が0.1だとして、effective batch sizeを普通の問題(画像認識だと100%想定)と同じ(32がデフォルト)にするには100倍(0.1^2)くらいで2048か4096で良いのでは？
- https://towardsdatascience.com/why-using-learning-rate-schedulers-in-nns-may-be-a-waste-of-time-8fa20339002b

学習の安定化

- lgbmでn_estimatorsを大きくするように、nnで学習を安定化する方法は無いか？
- 安定してかつ高速な設定で色々試すと効率良い気がする
- random seed avgやbaggingは安定するけど遅い
- https://arxiv.org/abs/2101.08286
- https://arxiv.org/pdf/2106.02100.pdf
- Figure 2: trainがcleanだとepochが大きくなると発散する。なぜ？

アイデア

- batch sizeをだんだん大きくしたら成績上がったかも
- double descentが発生するか確認 -> https://gist.github.com/richmanbtc/152eb9b3df812f7b0b59ee2b9840a526
- batch_size変えて同じ実験 -> https://gist.github.com/richmanbtc/5fbf86d7d8768ca8a8eac1d6e2349c46
- dropoutの影響は？ -> dropoutを無くしてもepochに対してval loss増えていく
- dropout 0.9 batch size4096 -> https://gist.github.com/richmanbtc/dd448890bdf3ee3981f4c025be20de22
- dropout 0.9 batch size32 -> https://gist.github.com/richmanbtc/0cc505918b3d5d920bb5dd5bbb1b7f7b
- dropoutは大きめが良いらしい
- dropout 0.9 + batch schedule -> https://gist.github.com/richmanbtc/e2b4bbe7740192d7f1f39bf09acf7aa1
- batch scheduleは要らない気がする
- dropout 0.9 batch size4096 + cnn size 12 -> https://gist.github.com/richmanbtc/db44e54ecd6603def155ec935d59273e
- dropout 0.9 batch size4096 + dense relu -> https://gist.github.com/richmanbtc/fa32a836923ae1b9b4dc8a947d78377e
- dropout 0.9 batch size4096 + dense relu dropout -> https://gist.github.com/richmanbtc/2313e0178ccbe9da8989927686b5f3d1
- サイズ増やせば増やすほど性能上がる？
- input dropout 0.5 -> val loss増える
- epochに対してval lossが単調減少になるようにするにはどうすれば良い？
- lrを変えてもval lossの形は変わらなそう (https://arxiv.org/pdf/2106.02100.pdf)
- regularizationが有効な気がする (https://arxiv.org/pdf/1908.05355.pdf)
- https://arxiv.org/pdf/2003.01897.pdf
- https://www.jstage.jst.go.jp/article/pbsj/49/0/49_28/_pdf
- l2 regまたはdropoutでdouble descent防げるらしい
- l1 regやl2 reg単独ではdropoutよりも成績悪い

optimal dropout

https://www.jstage.jst.go.jp/article/pbsj/49/0/49_28/_pdf
4 The Optimal Hyperparameter to Eliminate Double Descentによると
c = p / nが小さい時は0.5, 大きい時は1に近づくらしい
https://www.wolframalpha.com/input?i=%281+%2B+2+*+sqrt%28x%29+%2B+x%29+%2F+%282+%2B+2+*+sqrt%28x%29+%2B+x%29+from+0+to+10
nnだと入力はcが小さく、中間層はcが大きくなりそう
nnだと複数の中間層に情報を伝えておけば、dropoutされても情報が消えないから、高めが良い気がする
入力だけ0.5でdropoutしたらどうなる？
そもそも特徴量間に相関があるから、入力も高めが良いかもだけど
論文だと直交行列でp次元からk次元にしている。
実際は直交じゃないことが多いから、高めが良い気がする

val lossが単調減少になれば、
valを確保しないで十分なepochで学習すれば良くなるから、
学習データ増える
サンプル数増えるとdouble descentは発生しづらくなるらしいから、
valを確保したケースで単調減少なら、全体でも単調減少になりそう

良さそうな設定

- arch: dense l1, 1dcnn(2d)
- dropout 0.9 (val lossが単調になる)
- batch size 4096 (速いだけ)
- loss: sign mse + sw, double sharpe

1dcnnの分析

- https://twitter.com/colun/status/1484607710369021952

アイデア

- 1dcnnを重ねたらどうなる？ -> https://gist.github.com/richmanbtc/c346c6868663ac34f12642a94b7ac091
- 1dcnnを重ねて深くしたら？ -> https://gist.github.com/richmanbtc/4263a14c75d95d2390a5bd1110c6026f
- 1dcnnを深くしたらどうなる？ -> https://gist.github.com/richmanbtc/361e89afc5eaba4702df368e4bbfdf66
- 1dcnn(1d)をチューニングしたら1dcnn(2d)に匹敵するか？ (2Dが重要かサイズが重要か)
- もし2Dが重要だとしたら、特徴量の種類、特徴量の時間軸が軸になっている？

custom loss

- dropout 0.9 batch size4096 + custom loss -> https://gist.github.com/richmanbtc/f663e7f2e46ef8dce722db65cdfc07ad
- dropout 0.9 batch size4096 + custom loss2(sharpe) -> https://gist.github.com/richmanbtc/848292bc75a3b90f3ef12e88d046a86e
- dropout 0.9 batch size4096 + custom loss3(double sharpe) -> https://gist.github.com/richmanbtc/78d08c1495b94bf5983312b19a7320f9
- dropout 0.9 batch size4096 + custom loss3 shuffle=False -> https://gist.github.com/richmanbtc/d709bc1241f15afcad0df4c03ca5e643
- dropout 0.9 batch size4096 + custom loss2 + output linear -> https://gist.github.com/richmanbtc/056852d29b894211cef51b928b5867c7
- custom lossのときはtanhではなくlinearが良いかも
- custom lossは収束が遅い気がする。epochを増やしてみる
- dropout 0.9 batch size4096 + custom loss2 + 40 epoch -> https://gist.github.com/richmanbtc/04555e33ee7af54d95701d0b4c633420
- dropout 0.9 batch size4096 + custom loss3 shuffle=False + output linear -> https://gist.github.com/richmanbtc/790030b2f5c3b8fc3b432c6dc66af5b8
- feature exposureをlossに組み込めないか？
- 2値分類系のlossだとどうなる？
- binary_crossentropy -> https://gist.github.com/richmanbtc/8dca0d8b22a9ec70db17efac738bc526
- binary_crossentropy + sample weight 1 -> https://gist.github.com/richmanbtc/bda8e7688aac9220553b12813d6b9d6a
- hinge -> https://gist.github.com/richmanbtc/474c83584b46950791edf52a444f2609
- squared_hinge -> https://gist.github.com/richmanbtc/5cb11534ba39ca5e92dfea9082ea3441
- sample_weightを時期ごとに正規化したら？ (重み付けされていない評価指標がepoch後半で改善している気がする。ボラが小さい時期が後で最適化されているのでは？時期ごとに正規化すれば同時に最適化できるのでは？)
- sign mse + sample_weight 時期ごとに正規化 -> todo
- simple mse -> https://gist.github.com/richmanbtc/8a567b1a22e2e14bb3e4dfcb47144cce

mseやbinary_crossentropyなどの一般的なlossだと、
最適化しやすいとか、val lossが単調減少になりやすいとか、
そういう性質がありそう
custom lossで全期間で安定させるよりも、
まずは一般的なlossを減らすほうが良い気がする

dropout 0.9 batch size4096 + cnn size 12のdouble sharpeが高い件

- 仮説1: double descent的に性能が上がった？
- 普通のケースと性能の上がり方が違うとか？
- epoch後半でlossへの寄与はあまり大きくないがdouble sharpeへの寄与が大きい方向の最適化がされたとか？
- epoch 100とかで検証できそう？
- dropout 0.9 batch size4096 (追試。最後まで学習): https://gist.github.com/richmanbtc/e44cc6a55f6c74d41cd483b2b6ed69f2
- dropout 0.9 batch size4096 + epoch 100: val loss発散
- dropout 0.95 batch size4096 + epoch 100: val loss発散
- dropout 0.9 batch size4096 + epoch 100 + es: https://gist.github.com/richmanbtc/1a928615bd46d27cfc47a63e0b0ff2d7
- partial_fitと結果違うかも？optimizerの挙動が変わる？partial_fitは意図せずsgdになってたかも
- random_stateが毎回同じ値が使われるからっぽい。partial_fitで毎回random_stateを変えたら似た挙動になった
- 毎回同じ並びで学習することが正則化みたいになっていてval lossが単調減少っぽくなっていたのかも
- 毎回shuffle + 正則化を強くすれば良い？
- cnnだと正則化強くしても発散するけどdenseだと発散しなくなった。重みが多いから？ https://gist.github.com/richmanbtc/2771b354f59a218f0c77b61cdb51e7ba
- cnnの正則化だけ大きくしたほうが良い？ -> ぱっとしない
- denseでもsize 512だと発散した
- 仮説2: dropoutの特徴？sharpeが低いのでdropoutでsharpeが低くなる代わりにdouble sharpeが上がっているのでは？
- 線形回帰でridgeと比較すれば検証できそう？
- 仮説3: 微小なl2正則化が効いている？ (ridgeless的な)
- 後半に少しずつval lossが減るのは正則化のせいでは？
- 最適化を加速するために、l2正則化をだんだん小さくしたらどうなる？
- https://arxiv.org/abs/2011.11152
- >For example, the optimal weight decay value tends to be zero given long enough training time.
- ridgelessが良いということ？
- dense sparse dense学習も関連している？
- language modelだとl2 + adamが良いらしい。問題によって変わるのか
- https://towardsdatascience.com/why-adamw-matters-736223f31b5d
- >This means that L2 regularization does not work as intended and is not as effective as with SGD which is why SGD yields models that generalize better and has been used for most state-of-the-art results.
- 知らなかった
- https://arxiv.org/abs/1711.05101

weight decay

- adamw (1e-7): 
- adams (1e-7):
- l2 (1e-5): 
- adamw (1e-5):
- adams (1e-5):

同じrandom seedで学習

- サンプルの順番に加えてdropoutも同じパターンになるかも
- 同じパターンで学習すると成績良い？

l2正則化とweight decay

- cnnだと正則化強くしても発散するけどdenseだと発散しなくなった。重みが多いから？ https://gist.github.com/richmanbtc/2771b354f59a218f0c77b61cdb51e7ba
- cnnの正則化だけ大きくしたほうが良い？ -> ぱっとしない
- dense 512 + weight decay 1e-3: val loss発散
- weight decay 1e-2だと重みがゼロに収束している気がする (val lossが変化しなくなるので)
- 仮説1: relu + 強い正則化で勾配消失？
- 仮説2: reluで勾配消失すると発散しやすい？
- leaky relu + weight decay 1e-2 + dense 512 (仮説1の検証): https://gist.github.com/richmanbtc/252ff6c95403fb1f8deb85909abd3053
- leaky relu + weight decay 1e-3 + dense 512 (仮説2の検証): https://gist.github.com/richmanbtc/46213ddad45eb9de5f4f1db1635df90d
- 仮説2は違うっぽい。勾配消失とは無関係に正則化が弱いと発散
- https://arxiv.org/pdf/2206.01378.pdf
- early stoppingやreduce lrはl2 regと相性が良い気がする。val lossに含まれていないと単調減少にならない
- 発散しないのは無理なのかも。小さすぎないepochでesするほうが良い？
- leaky relu + dense 512 + weight decay 1e-3 + es: https://gist.github.com/richmanbtc/0e71a77bfa933dde5420fe0a6a2aa3f4
- relu + dense 512 + weight decay 1e-3 + es: https://gist.github.com/richmanbtc/76b92af92571ca244e12b59234bcb287
- https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd
- weight decayは意味がわかりづらいから、l2のほうが良い気がする
- 1dcnn(2d) + l2 1e-3: ing
- https://arxiv.org/pdf/2108.12006.pdf
- ラベルノイズが十分大きい場合、必ずESが必要になるらしい
- でも、正則化をかけたときにどうなるかは分析されていないか
- 手元の実験だと、正則化をかけてもNDD-ESになるからES必要そうだけど
- https://mohammadpz.github.io/DD.html

esと正則化

- 正則化無しでesすると、ランダムな場所からtrainにfitする領域に射影する感じになる？
- nnはtrainにfitする領域がたくさんあるけど、そこからランダムにサンプルする感じになるのでは？
- 一方、正則化をかけるとその中の1点に寄せていく感じになるのでは？
- 初期にそこそこ良い解に行って(まだ正則化の分のlossが大きい)、一度劣化してから、再度良くなるのを観察して思った
- そこそこ良い解は射影した解で、再度良くなるのは正則化の解？
- また、valに誤差がある。正則化があると局所解の数が減るからvalの試行回数が減って、信頼区間の上限が改善するのでは？
- このこと(bestを選ぶことでバイアスがかかる)はあまり分析されてないような気がする
- 同じようなval lossでもバックテスト結果にかなり差があるのを見て思った
- https://arxiv.org/pdf/1703.09580.pdf

random_seedを間違えていた件

- 同じrandom_seedでpartial_fitをしていた
- shuffleしていない影響が大きいかと思ったが、それよりもdropout固定の影響が大きいのでは？
- dropout固定でl2 regで学習すると、dropしたunitにつながる重みはゼロに向かう
- dropしていないunitのみで学習することになる。これが効いていたのでは？
- でもバッチ間ではdropoutが変化するのか。同じサンプルに対して同じdropoutにはなる
- 偶然新しい手法見つけた？
- ちゃんとやるならshuffleしない(または期間ごとにshuffle)で同じrandom_seedでやると、batch間の独立性が高まって良いかもしれない

batch size

- 大きいbatch sizeやfull batchでやると、val lossが小さい期間が長くなる気がする
- これならepoch固定でも良いのでは？
- full batch: https://gist.github.com/richmanbtc/add524bfae7bd53a4f337d620346e407

optimizer

- https://arxiv.org/pdf/2108.11371.pdf
- adamは勾配が小さくtrain errorも小さい点を見つけるが、test errorが大きいらしい
- weight decayがあったとしても
- adamの学習率とmomentumのどちらが影響している？
- adamの学習率はelement wiseで調整するからそれが影響してたりするのかな
- https://pub.towardsai.net/why-adam-optimizer-should-not-be-the-default-learning-algorithm-a2b8d019eaa0
- これによると学習率とmomentumのどちらも影響していそう
- https://openreview.net/forum?id=D1TYemnoRN
- https://arxiv.org/pdf/2202.10670.pdf
- https://arxiv.org/pdf/2206.03299.pdf
- https://arxiv.org/pdf/2010.11924.pdf
- 初期重みからの距離も分析されている。
- pacbayes.mag.flatnessが効くなら、重みにgaussノイズを加えると良い？

distance l2 + nadam: https://gist.github.com/richmanbtc/d227eba16fb18a3a54fc16b58207d11a

試す

- rank gauss: https://gist.github.com/richmanbtc/08d031006e5000e63efa70c2b296da1f
- rank uniform: https://gist.github.com/richmanbtc/ea55db58849887741513473b2cab8fd8
- rank uniform + sc: https://gist.github.com/richmanbtc/63121669239a58785bd8a390612432a5
- bagging vs random seed avg vs large nn
- cv early stopping
- bagging (128 * 10, shuffleされているのでvalがリークしている): https://gist.github.com/richmanbtc/fdaa726794e4f95ebdf90caeffe36016
- random seed avg (128 * 10): todo
- large nn (512): model size 9MB https://gist.github.com/richmanbtc/798bc413734799561284d1993f356147
- large nn (512) + l1 reg: model size 9MB https://gist.github.com/richmanbtc/038cf759f7b6b3349673cdb38eb98225
- large nn (512) + quantize (16bit): model size todo
- large nn (1024): https://gist.github.com/richmanbtc/1b1ebf1a32940a270b337bf8decb9ec8
- large nn (512, layer 3, dropout 0.8, weight decay 1e-3): https://gist.github.com/richmanbtc/2c2f144214e697fb882d96dbb31f3649
- weight decay + custom loss(sharpe, shuffle true) + linear act: https://gist.github.com/richmanbtc/012fc090f35c46d251e5b4cde67dfed5
- dropconnect (重み絶対値比例ノイズ。cnnで効く気がする) https://github.com/CyberZHG/keras-drop-connect/blob/master/keras_drop_connect/wrappers.py
- adamS (pytorch)
- sample weight schedule (最初はフラット。複数ロスで実装)
- custom loss schedule
- early stopping by grad: https://arxiv.org/pdf/1703.09580.pdf
- rank gauss + feature remove: https://gist.github.com/richmanbtc/2d9c598df8bab07a1d9eabe0a85bad38
- rank gauss + distance l2 (projection) + adamw: https://gist.github.com/richmanbtc/bc92e51509b69899a9a8e53c16f35f48
- distance l2 (projection) + adamw: https://gist.github.com/richmanbtc/de28cfa4969f641902cb5d894e384bf7
- sample weight epsilon (1e-1): todo
- sample weight epsilon (1e-2): https://gist.github.com/richmanbtc/a160bdc89df4e6827ca25391328cdc11
- sample weight rolling normalize: https://gist.github.com/richmanbtc/860f222c9cc792e7e6890d7c8728cd61

効いたものメモ

- distance l2 + nadam
- weight decay + adamw
- 1dcnn(2d) + fixed random seed
- early stopping
- lr warmup
- rank gauss (val lossは良くないがバックテストが良い)
- output rescaling

ensemble vs large

- https://arxiv.org/pdf/2202.06985.pdf

weight initialization

- https://cvml-expertguide.net/terms/dl/optimization/weight-initialization/
- https://arxiv.org/pdf/1607.02488.pdf
- 出力直前でrescaling(0.1) <- かなり効いた(というかweight decayを小さくしても最適化できるようになったから、weight decayを小さくした効果？)
- rescalingは完全にマッチするよりも大きめが良い？
- dropoutの補正は効かないかも？
- 最終レイヤーはbias=Falseが良いかも？
- 全レイヤーbias=Falseのほうが成績良い気がする (他のパラメータ次第っぽい)
- 初期化方法とrescalingは重要なパラメータっぽい
- 初期値改善したらlr warmup要らないかも

dropout

- 最後の層に0.99で入れるのが良いらしい
- gaussian dropoutでないと0.99のような大きいのは入れづらい
- 最後の層以外に入れると学習がうまくいかない気がする

feature exposure

- 層の数を増やすとfeature exposureが下がる
- 入力にgauss noise加えて学習させるとfeature exposure下がる

resnet

- resnet的にすると性能上がる。あと学習が安定する気がする
- でもバックテストはあまり良くないかも

weight decay

- 強くかけた解と、軽くかけた解がある気がする
- 強くかけた解は多分強制的に正則化されたlossのminに収束した解で
- 軽くかけた解は初期値から近いminに収束した解な気がする
- 実験的に、weight decayは大きいとfeature exposureが大きくなる

val lossを見ながら試行錯誤するとデータを消費してしまうから
根拠ベースでチューニングしたい

根拠のありそうなもの一覧 (他の人の報告)

- 最適化の距離が小さいほうが汎化性能が高い (https://arxiv.org/pdf/2202.10670.pdf)
- rescalingとweight initを調整すると最適化の距離を小さくできる
- learning rateを小さくすると初期重み距離と最適化距離が小さくなる (自分の実験)
- pacbayes.mag.flatnessが大きいほうが汎化性能が高い (https://arxiv.org/pdf/2010.11924.pdf)
- 重みに乱数を掛け算すればpacbayes.mag.flatnessを考慮したlossにできる
- weight decayとesはどちらも使った上で、weight decayをチューニングすると良さそう (https://arxiv.org/pdf/2206.01378.pdf)
- mishが欠点が少なそうだけど、ケースバイケースが答えな気がする。 https://arxiv.org/pdf/2109.14545.pdf
- l2sp https://arxiv.org/pdf/2002.08253.pdf

根拠のありそうなもの一覧 (自分の仮説や今までの実験から)

- last layer gaussian dropoutは、yの分布をうまく表現できそう
- custom loss(double sharpeとか)は効かなそう (実験的にも。lossの式から直感的にも)

根拠不明のもの一覧 (今の所val lossでチューニングするしかない)

- どのactivationが良いか？ -> 無難に有名なreluにしておくか。he initはrelu想定だし
- 実験的にはeluが良い気がする
- weight decayはどのくらいが良いか？
- ユニット数はどのくらいが良いか？
- 層数はどのくらいが良いか？
- dropout vs gaussian dropout
- dropout率はどの程度が良いか？ -> 最終ユニット数で調整が良さそう (多分、最終ユニット数が多いとdropoutの分散が減って過学習しやすい)
- rescalingはどの程度が良いか？
- last layer dropout vs all layer dropout
- standard scaler(with mean) vs standard scaler(without mean) vs rank gauss
- どのweight initが良いか？ https://arxiv.org/pdf/2001.05992.pdf

sgd vs adam

- https://proceedings.neurips.cc/paper/2020/file/f3f27a324736617f20abbf2ffd806f6d-Paper.pdf
- 最適化の距離はsgdは大きくなりがち、初期重みからの距離とか最適化距離以外の要素もある気がする

pacbayes.mag.flatness

- これを最小化するために、重みをランダムに変えるDense作った
- -> 効くっぽい。あと学習が遅くなる。でもes patient大きくしたら成績落ちたかも。必ず効くわけではなさそう
- また、初期重み距離が大きくなる気がする
- val lossをこれで計算してみたい
- 初期重みからの距離正則化 -> 効いた
- feature exposure loss (y_pred * x[i])^2
- https://arxiv.org/pdf/1901.04653.pdf
- https://arxiv.org/pdf/1912.02178.pdf

learning efficiency

- val_lossの変化量をweight_dist, weight_lenで割る
- lossが凸ならval_loss/weight_len最大化の解は、learning rate +0のSGD (多分)
- weightの大きさに依存させないために、weight_dist / |weight_init|で割ったほうが良いかも
- train lossでlearning efficiencyを見積もれればvalに依存しなくて良い？

learning efficiencyを高くするためにできること

- 最適化の方法を変える (sgd vs adam, lr, batch size。小さいlr + sgdかnadamが良さそう)
- 初期値とoutput rescalingを変える
- weight_dist正則化をかける
- アーキテクチャーを変える
- activationでも変わるらしい

初期重み距離とgeneralize error

- https://arxiv.org/pdf/2002.08253.pdf
- https://arxiv.org/pdf/1802.01483.pdf
- https://arxiv.org/abs/1901.01672
- theorem 2でレイヤーが分散を変えずに伝えれば実質的にDの和で決まる気がする。l2spが効く理由はこれでは？

dropoutで初期重み正則化

- https://arxiv.org/pdf/1909.11299v1.pdf
- mixout vs l2sp https://arxiv.org/pdf/2107.04835.pdf
- https://arxiv.org/pdf/2206.05658.pdf
- dropout rateは0.5が良さそう (実験的にもpacbayes.mag.flatnessとの関係的にも)
- 単純にノイズが増えて精度が上がってる説を検証するために、mixout無しでbatchサイズ変えたものと比較
- todo

最終レイヤーの重みをゼロ初期化したらどうなる？
rescalingが不要になる気がする
-> 性能落ちた。ランダム初期化が良いらしい

実験で思ったこと

- lrを小さくするのが性能に一番効く (weight_distが小さくなるoptimizer)
- mixout(weight)が効く。pacbayes.mag.flatnessと関係してそう
- これらはgeneralization gapと関係しているらしい指標と関係しているから
- generalization gapが減る影響が大きいのかも

mars constraint

- 効くときもあるけど不安定な気がする。他パラメータと相互作用して結果が一貫しない

重みを分布として見る見方

- http://ibis.t.u-tokyo.ac.jp/suzuki/lecture/2020/intensive2/KyusyuStatZemi2020.pdf
- 周波数が制限された無限のユニット数で見るなら、層ごとにactivationの前後で平滑化したらどうなる？(エイリアシングノイズ除去を意図)
- または、weight = FFT * 低周波のみの重み * IFFT
- 名付けるなら、unit upsampling?
- smoothingしても性能ほぼ変わらなかったから、実質の重みを1/4に減らせている
- 逆に実質の重みを変えずにsmoothingしたら性能落ちた
- https://arxiv.org/pdf/1906.10822.pdf
- mixout(weight)に似てるかも。gradientでノイズを作るか初期値からのずれでノイズを作るか

TODO

- 出力を模索 (gaussian dropoutよりよいものはあるか？)
- self supervised模索

出力について

- gaussian dropoutは本当に必要か？(小さいバッチ, 小さいlr, mixoutなどと組み合わせたら不要になったりしないか？)
- 単純に、勾配全体にgaussノイズを掛け算しているような感じでは？
- 単純にgaussian dropoutを除いたら、lrやバッチ変えても性能落ちた(val lossが悪い。sharpeは意外と悪くないかも)。なんらかのノイズは必要らしい
- gaussian dropout無し + mixoutもだめだった
- 最終出力にgaussian dropout(0.5)をやったら学習できるようになった
- gaussian dropout(0.2)だと過学習。gaussian dropout(0.9)だと学習がかなり遅い
- gaussian dropout(0.5)だとunit数変えても学習できる (val lossは低い。sharpeがどうなるかは試してない)
- gaussian dropout(0.5)のlossの期待値は、元の問題と変わらないから(スケールは少し変わるけど) https://gist.github.com/richmanbtc/e7e00fd90f21c5fe00d8f607bdfb567e
- これはoptimizerに影響を与えている気がする
- output gaussian dropout(0.5)のほうが性能良いかも
- output dropout(0.5)のほうがさらに良いかも
- https://arxiv.org/abs/1810.01322
- https://openreview.net/pdf?id=HkCjNI5ex
- https://arxiv.org/pdf/1512.00567.pdf
- https://cvml-expertguide.net/terms/dl/regularization/label-smoothing/
- 出力を正則化したらどうなる？ -> 性能悪い
- 単純にgaussノイズを出力に足す -> 性能悪い
- よく考えたらdropoutの場合は、train時に学習サンプルを間引くのと、2倍してるだけか
- 試しに、train時のみ2倍してみたら、val loss下がった。dropout意味無かった
- y_trueを-0.5, 0.5で学習させるのと同じか。label smoothingの仲間な気がする
- https://en.wikipedia.org/wiki/Shrinkage_(statistics)
- last layerにL2正則化かけたけどうまくいかなかった
- nnだと全てのlayerでスケールしうるからlast layerだけだとうまくいかないのかも。
- 一方全てのlayerに正則化かけると、それ以外の効果も出てしまう気がする
- >Shrinkage is implicit in Bayesian inference and penalized likelihood inference
- dropout, gaussian dropoutがうまくいく理由はこれかな。dropoutの場合は実質shrinkageになる
- http://proceedings.mlr.press/v80/imani18a/imani18a.pdf
- soft targetとも呼ぶのか。よく知られた話らしい
- shrinkageが強いとdouble sharpeが上がる気がする
- cosine_similarityで評価すると、全てのshrinkageの最適値で評価できる
- L2正則化かけた場合はcosine_similarityのほうが成績良いっぽい
- でも不安定だからmseが良さそう

学習サンプルを選ぶ方法

- https://proceedings.neurips.cc/paper/2020/file/3a077e8acfc4a2b463c47f2125fdfac5-Paper.pdf

loss

- sign return: mse, binaryentropy(label smoothing)
- raw return: mae, logcosh

重み初期化

- スパースに初期化
- 実際に計算して分散を1にする
- https://arxiv.org/pdf/2102.01732.pdf
- self supervised
- https://arxiv.org/abs/2209.15283
- https://arxiv.org/abs/2008.02965
- https://ai-scholar.tech/articles/adversarial-perturbation/Earlystopping
- eluの成績が良い理由？
- https://twitter.com/richmanbtc/status/1580778758633828355
- https://arxiv.org/pdf/2110.00653.pdf
- 過学習させたあとでランダムな直交行列のみ学習する？
- https://arxiv.org/pdf/1902.06853.pdf
- EOC試してみたい
- シャッフル学習で重みをゼロ初期化するとどうなる？
- https://arxiv.org/pdf/1709.02956.pdf
- https://proceedings.neurips.cc/paper/2019/file/e520f70ac3930490458892665cda6620-Paper.pdf
- https://arxiv.org/pdf/2107.09437.pdf

効きそうなもの

- 初期値のスパース化
- 初期値の分散をレイヤー間で調整する
- gelu activation (試したら良かった)
- adabelief (weight_len / weight_distが小さい気がする。nadam 2e-4も良い気がする)
- L2正則化かweight decayが実は効くかもしれない (l2spではなく)
- L2正則化で(weight_len / weight_dist)が小さくなる気がする。
- weight_distは増えるが、weight_lenの増えは大きくない
- L2正則化は一貫性が無いかも。大きく劣化するときがある
- beta_1を0.5とかも良かった気がする (実質のバッチサイズが変わっているだけ？)
- L2が効く理由は、レイヤー間の勾配の大きさのバランスが変わるから？
- L2正則化をかけると、前半のレイヤーの重みが小さくなる -> 前半の勾配が相対的に大きくなる
- rmsprop系ってレイヤー間の学習率自動調整されるのかな
- L2正則化がかかるとrmsprop系のepsと同じ効果がある気がする (小さすぎる勾配が無視される)
- L2正則化の代わりにepsを大きくしたらどうなる？ -> ぱっとしない
- L2正則化で重み初期値がちょうど良い値になっている？(he_normalで初期化 -> eluだとchaos領域で初期化されるのでは？それがマイルドになるとか)
- 少しchaos寄りにしてL2でorderedにするのが良い？(2層だとhe_normalで良いけど6層だとL2 lossが大きすぎて最適化できなかったので)

l2sp(proj)試してなかった -> ぱっとしない

momentumとbatch size

>In the original paper introducing U-Net, the authors mention that they reduced the batch size to 1 (so they went from mini-batch GD to SGD) and compensated by adopting a momentum of 0.99. They got SOTA results, but it's hard to determine what role this decision played.
https://stats.stackexchange.com/questions/201775/dealing-with-small-batch-size-in-sgd-training

SGDでmomentumとbatch size調整してみるか

memo
https://gist.github.com/richmanbtc/21bdf8a640b5e380754b51d4da1f69ce

特徴量

- standardscaler + rank gaussが効いた
- pca(whiten)は不明だけど一応入れておくか
- 曜日と月と日付を入れてみたい

sharpe 0.21781098730522866
double sharpe 1.663214329919795
https://gist.github.com/richmanbtc/86f1b58da5ddf0e1acf62fb8ff836ca3 small lr
https://gist.github.com/richmanbtc/ef4495d747253071d5c27cabe8ab332c good
https://gist.github.com/richmanbtc/efb4f5c6e4e5583f5d92f393ec1d1900 batch 512

optimize + n quantile 100
https://gist.github.com/richmanbtc/eb15943d67e60bea2ec450945259a401
n quantile 100
https://gist.github.com/richmanbtc/899e0729fcff85b1e45ed4ad7be4e816
tflite(no optimize)
random seedが違って結果が一致しなかった

optimize + n quantile 1000
https://gist.github.com/richmanbtc/ed6daf36b3d107b274d5c079f2e24d93
optimize + n quantile 100
https://gist.github.com/richmanbtc/27bf382eeebbd058e080a620634fe4ec
optimize + n quantile 100 + ensemble 3
https://gist.github.com/richmanbtc/61052948a24073b17baa9f513100d598
optimize + n quantile 100 + ensemble 3 + lr0.001/8
https://gist.github.com/richmanbtc/4471cc1086fd6bc75d24926353fc1bb6
optimize + n quantile 100 + ensemble 3 + lr0.001
https://gist.github.com/richmanbtc/afe475f1880767c3ea2e2dfe97b9e35a
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + mixout(weight) + sc only
https://gist.github.com/richmanbtc/1f1bb372c908be0a0ee7f2d96862b5b1
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + sc only
https://gist.github.com/richmanbtc/13b5dca338b059f7bf21c350229379a0
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + all features
https://gist.github.com/richmanbtc/c43e9e40d298d9cf77e80e564736c370
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + sc only + 64unit
https://gist.github.com/richmanbtc/77f678c72f39081447d571fb30d643f5
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + sc only + 256unit
https://gist.github.com/richmanbtc/f1f2e32390ac699eecc0e02ec1e344d1
optimize + n quantile 100 + ensemble 10 + lr0.001/8/8 + sc only
https://gist.github.com/richmanbtc/4b128fab713800288f295f776c6ac1df
optimize + n quantile 100 + ensemble 10 + lr0.001/8 + sc only + batch size4096*8
https://gist.github.com/richmanbtc/5275f155527ae6d7d2232c3aa6d203e9
optimize + n quantile 100 + ensemble 80 + lr0.001/8 + sc only + batch size4096*8
todo (notebook renameしてデプロイ)

- ensembleするとmixoutはほとんど意味無いらしい (少しdouble sharpeが上がっている？)

todo

- 保存できない問題を修正 -> callbackを消したら動いた
- モデル圧縮を試す -> 動かすの大変そう
- 同じファイルサイズのアンサンブルのときに、モデル圧縮を使った場合と使わない場合で、性能比較
- preprocessorで2MBくらいになる問題調査 -> quantiletransformerか
- kerasで2MBくらいになる問題調査 (試算だと無圧縮で700KB程度ははず) -> tfliteで200KBに圧縮できた
- 圧縮で性能劣化しないか確認 -> 劣化しなかった
- 3 ensembleで性能確認 -> 劣化した。random seedの影響が大きいらしい
- 本番モデル学習 (40 ensemble)

random seed

- https://arxiv.org/abs/2109.08203v1
- https://proceedings.mlsys.org/paper/2022/file/757b505cfd34c64c85ca5b5690ee5293-Paper.pdf
- 結構ばらつくのと、val lossとcv結果が相関している気がするから、random restartで選んだほうが良いかも
- 200学習して上位40でアンサンブルするとか

https://papers.nips.cc/paper/2020/file/e4191d610537305de1d294adb121b513-Paper.pdf
https://openaccess.thecvf.com/content_CVPR_2020/papers/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.pdf

random seed avg, random restart

- アンサンブルした結果で比較して、正則化やdropoutは本当に効果がある？
- 学習のばらつきを抑えるものと、本質的に汎化性能に寄与するものがある気がする
- ある程度バリアンスが小さく無いとearly stoppingが機能しない気がする
- ensemble early stoppingはありえる？ (複数モデルを同じepoch学習させて、アンサンブル結果でes)
- データからくるばらつきと、random seedからくるばらつきのどちらに効くか？
- 勾配の分散(SGDでimplicit regularizationされる)がデータからくるばらつき？
- 勾配の分散に対する正則化効果があるものは、random seed avg後でも有効なものでは？
- mixoutは多分勾配の分散を抑える効果がある
- mixoutはSGDのimplicit regとかぶらないか？

ensemble

- https://arxiv.org/pdf/1802.07881.pdf
- https://www.ijcai.org/proceedings/2018/0301.pdf
- https://www.microsoft.com/en-us/research/blog/three-mysteries-in-deep-learning-ensemble-knowledge-distillation-and-self-distillation/

generalization gap予測

- https://arxiv.org/pdf/1810.00113.pdf
- regressionだとnewton raphson的な式になるのかな
- dropoutがこれを減らす気がする
- 中間層の情報が予測に寄与するらしい
- SGDはweightの勾配の分散を減らす効果があるけど、

ノイズの多いデータでES不要にできるか？

- https://proceedings.neurips.cc/paper/2021/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf
- 確率分布を出力するタイプのnnなら、ノイズが減って、ES不要な領域を作れるのでは？
- https://wazalabo.com/mixup_1.html
- https://arxiv.org/pdf/2204.03323.pdf

思いついたこと1

- 同じクラスのみでbatchを作ったらどうなる？
- SGDのimplicit クラス間の勾配の違いを強調する
- 実験したらぱっとしなかった

思いついたこと2

- バッチサイズをサンプルサイズ以上に上げたらどうなる？
- momentumを上げることで実質的に上げられないか？

overparameterization 

- error分解
- http://proceedings.mlr.press/v119/d-ascoli20a/d-ascoli20a.pdf
- Figure 8: アンサンブルすると正則化がいらなくなる？
- https://openreview.net/pdf?id=B1g5sA4twr
- https://proceedings.neurips.cc/paper/2020/file/e0ab531ec312161511493b002f9be2ee-Paper.pdf

overparameterizationやってみる

cnn実験 (条件はコード)

- https://gist.github.com/richmanbtc/0ed02f9327219c1c77e2900105d2270c
- https://gist.github.com/richmanbtc/31683f0ed83e9c6e206c05a9a22aae19
- https://gist.github.com/richmanbtc/c65a72531d1c0e866aea794d32bfdd06
- https://gist.github.com/richmanbtc/5508dcd087c47bb8f8fe1b7fa5f5cde8

作業

- remove_featモデル(beta)をnnで学習 done
- rankモデルをnnで学習 done
- 追加の特徴量入れる (どのnotebookか忘れた) DONE
- btc単独 + スレッショルド -> ぱっとしない

ポジション計算

- 意外と影響大きかった
- 単一銘柄で模索して、良いものを複数銘柄に移植したい
- -> ぱっとしない

btc短期(4時間horizon) + cnnはぱっとしない

weight norm regularization

- https://arxiv.org/pdf/2107.09437.pdf
- これによると、hidden1層の場合、重みのノルムが汎化性能と関係するらしい
- 制約にしたらどうなる？
- https://arxiv.org/pdf/1911.07956.pdf
- https://arxiv.org/pdf/2102.03497.pdf
- https://arxiv.org/abs/1706.05350
- https://arxiv.org/pdf/2103.06583.pdf
- https://arxiv.org/pdf/1911.05920.pdf
- https://arxiv.org/abs/2202.10788
- https://arxiv.org/pdf/1810.12281.pdf
- jacobian normとの関係 

shap

- https://blog.kxy.ai/effective-feature-selection/#limits

done

- portfolioでmean varianceを試す (実際のデータでバックテスト) -> ぱっとしない
- 標準偏差の逆数はupとほぼ差が無い。risk parityとかもぱっとしない
- risk parity portfolio -> LSだからできないか
- 4h horizonなどを模索

todo

- betaモデルをcnnで学習
- rankモデルをcnnで学習
- beta + ranker
- btc one + ranker
- eth beta
- eth one
- market model
- usd/jpy, eur/usd, cny/usd
- nasdaq, dou, sp500
- gold, oil, 小麦
- onchain data -> ぱっとしない
- 雇用統計
- 金利
- fomc
- twitter, telegram, discord, google trend
- sentiment
- 予測サイト (tradingviewとかfearとか)
- https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_2J4GS1002/_pdf
- 時間ずらしを模索
- garch vola特徴量

makerモデル研究 (まずはインターフェース設計から)

- https://twitter.com/richmanbtc/status/1585516580783665153
- https://twitter.com/richmanbtc/status/1584975425461424128
- https://twitter.com/richmanbtc/status/1585303604357103616
- 20221025_mm
- 意外と遅延に弱いかもしれない。
- 取引所依存するのも気になる

4h horizon

- btc beta oneはbtc以外がコスト負けしない
- rankは全体的にコスト負けする
- remove featはその中間くらい。ぱっとしない
- 8h btc beta one cnn https://gist.github.com/richmanbtc/2299ba3b23d6de426e991d8a6a868260
- 4h btc beta one cnn https://gist.github.com/richmanbtc/1660f35e787279dfc8281871ca28deb1
- 8h btc beta one cnn prod: committed

時間軸アンサンブル

- 似たようなモデルでも意外と相関が低い
- 相場急変時は特に低い気がする
- https://twitter.com/richmanbtc/status/1586044217704742912
- ret_h6, ret_h12, ret_h24, ...を予測してそれの平均がアンサンブルリターンになるから
- 直感的に成績上がる気がする
- wavelet変換やPCAでret間の相関をゼロにしたらどうなる？ (multi output regression + PCA)
- -> pcaはぱっとしない
- weight sharingや同じモデルで複数のretを予測したらどうなる？ (特徴量同じケース、特徴量時間軸スケールさせるケース)
- 時間軸を細かくしてもあまり差が無い。 https://gist.github.com/richmanbtc/6e1ea34c80f7e7dad28c11330fab084f

universe

- https://help.ftx.com/hc/en-us/articles/360027668812-Index-Calculation
- binance, ftxに存在
- 売買代金がそこそこある
- 増やしてもぱっとしない

ensemble, domain generalization

- https://arxiv.org/abs/2110.10832
- 20221029_btc_one_nn_avg 効いたかも
- esは素の方でやったほうが良いかも

特徴量

- fear greedy
- target encoding or shift: month, day, hour, month day hour, mean, median, std

data

- from cex: trades
- tardis: open interest, liquidation, order book
- まずは1時間で統一
- すべてdbに入れる
- dbはpostgresqlかbigqueryか？
- 新しいデータを実装するときの流れ
- jupyterでデータクラスを実装
- cloudpickleで保存
- デプロイ

schema = fetcher.schema()
df = fetcher.fetch(min_timestamp=None)
df: timestamp index

弱いやつの重み減らす

- upはほぼequal weightと成績同じ
- upだと重み調整が難しい (分布変えればできそうだけど)
- upでフィルターかequal weightで重み指定で良さそう
- バックテスト指標が悪いものを除くと成績上がる
- フォワードの情報だけでオンラインにやるのはあまり成績上がらない
- 重みはバックテスト結果から手動調整が良さそう
- 重みはそんなに細かく調整したいことは無いし、sparseなほうが扱いやすい気がするから
- フィルターが良い気がする

以下の基準でフィルターしてみる

- daily sharpe > 0.15
- daily sharpe = sharpe / sqrt(horizon in days)
- m-20220921-ethはぎりぎり基準外だが、珍しい戦略なので入れた
- 改めて調べたらh12もdaily sharpe高い気がする

TODO

- 4hモデル増やす (betaとかnnとか (リターン3倍ならスケーラビリティー1/2で済む)
- 30分足のhorizon8を試す
- 色々なデータ試す
- bigqueryデータをjupyterから使えるようにする
- https://zenn.dev/wannabebotter/articles/e9feb71e8738e5
- そもそもデータ増やすと性能が上がる仕組みを用意する (今はテクニカル指標+lgbmが強すぎて、余計なものを混ぜると性能落ちたりする)
- nnと同様にlgbmで大きいlrは正則化効果があったりしないか？

feature agglomeration random subspace (fars)

- 20221106 fars
- extra trees + colsample_bytreeと同じくらいの成績
- 常に良いとかどういうときに良いとかわからないけど
- 従来のbestと同じくらい良い新しいモデルができた
